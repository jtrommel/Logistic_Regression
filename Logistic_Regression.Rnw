\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Logistic Regression}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=LR}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(gridExtra)
library(broom)
library(mcsm)
library(scatterplot3d)
library(verification)
library(ROCR)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\frontmatter
\chapter*{Logistic Regression}

\mainmatter
\chapter{Logistic Regression: theoretical background}

\section{Why and when do we need a logistic regression?}

Let us start with the situation where the response variable $Y$ can only have two (discrete) levels ''Yes" and ''No", or ''Dead" and ''Alive". Usually we asign the value ''1" to one outcome and ''0" to the other. Strictly speaking these should not be viewed as actual numbers. The qualitative response (''Dead" vs ''Alive") are coded in an \emph{indicator variable} $y$, which can only have the values ''0" and ''1". We start with only one controllable factor $x$. Let us say that for small values of $x$ the response is ''0", for large values of $x$ the response is ''1". This general description can still give rise to very different situations. It all depends on the region in between: we can go from a situation with a very clear distinction, a fuzzy middle or fuzzy all round (Figure~\ref{fig:distinct}).

<<label=distinction,fig=TRUE,include=FALSE, echo=FALSE>>=
df_clear <- data.frame(x=seq(-10,10,0.5),y=0)
df_clear$y <- ifelse(df_clear$x<0,0,1)
df_clear$y <- ifelse(df_clear$x==0,0.5,df_clear$y)
p1 <- ggplot(data=df_clear) + 
  geom_point(aes(x=x, y=y), color="red") +
    labs(title="Clear distinction", xlab="x", ylab="y") +
  JT.theme
set.seed(2017)
df_fuzzy_middle <- data.frame(x=seq(-10,10,0.5),y=0)
df_fuzzy_middle$y[17:25] <- rbinom(9,1,0.5)
df_fuzzy_middle$y[26:41] <- 1
p2 <- ggplot(data=df_fuzzy_middle) + 
  geom_point(aes(x=x, y=y), color="red") +
    labs(title="Fuzzy middle", xlab="x", ylab="y") +
  JT.theme
df_fuzzy_complete <- data.frame(x=seq(-10,10,0.5),y=0)
df_fuzzy_middle$y <- rbinom(41,1,0.5)
p3 <- ggplot(data=df_fuzzy_middle) + 
  geom_point(aes(x=x, y=y), color="red") +
    labs(title="Fuzzy all round", xlab="x", ylab="y") +
  JT.theme
grid.arrange(p1, p2, p3, ncol=3)
@

\begin{figure}
\centering
\includegraphics[width=200pt, height=200pt]{LR-distinction}
\caption{Some situations}
\label{fig:distinct}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

It is important to remember that the results of one experiment represent only one sample of all possible experimental results. When we have multiple experiments\sidenote{replicated experiments} we get different results each time (Figure~\ref{fig:intro}-left): the response variable $y_{i}$ is a probability variable $Y_{i}$. For small and large values of $x$ the results will practicaly always be ''zero" (small values) or ''one" (large values). However, in the intermediate zone, sometimes $Y_{i}=0$ and sometimes $Y_{i}=1$. By taking the average of all results we get the black dots in Figure~\ref{fig:intro}-right. When we put a smoothing line through these averages we get the S-shaped blue line.

<<label=intro,fig=TRUE,include=FALSE, echo=FALSE>>=
N <- 10
n <- 10
b0 <- 0
b1 <- 0.7
set.seed(2018)
intro <- data.frame(expnr = rep((1:N), each=(2*n +1)), x = rep(seq(-n,n),N), y=0)
intro$p <- 1/(1+exp(-(b0+b1*intro$x)))
for (i in c(1:nrow(intro))) {intro$y[i] <- rbinom(1, 1, intro$p[i])}
intro %>% group_by(x) %>% summarise(mean(y)) -> gem
p1 <- ggplot() +
  geom_jitter(data=intro, aes(x = x, y = y, color=factor(expnr)), width = 0.01, height = 0.01) +
  xlim(-1.1*n, 1.1*n) +
  labs(title = "Experimental resuls", 
       subtitle = "10 repeat experiments",
       x = "independent variable x", 
       y = "observation of the responsvariable y") +
  JT.theme
p2 <- ggplot() +
  geom_jitter(data=intro, aes(x = x, y = y, color=factor(expnr)), width = 0.01, height = 0.01) +
  geom_point(data=gem, aes(x = x, y = `mean(y)`)) +
  geom_smooth(data=gem, aes(x = x, y = `mean(y)`), method=loess , se=FALSE, span=0.1) +
  xlim(-1.1*n, 1.1*n) +
  labs(title = "Experimental results", 
       subtitle = "10 repeat experiments",
       x = "independent variable x", 
       y = "observation of the responsvariable y") +
  JT.theme +
  theme(legend.position="none")
grid.arrange(p1, p2, ncol=2)
@

\begin{figure}
\centering
\includegraphics[width=200pt, height=200pt]{LR-intro}
\caption{Many experiments}
\label{fig:intro}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

\section{Why can't we use a univariate linear regression?}
When the response variable $Y$ is \emph{qualitative} rather than \emph{quanti\-tative}, linear regression is not the best method to link $Y$ with the controllable factors $x_{i}$\sidenote{Y (in capital) is a probability variable, x (lower case) is known}. 

\newthought{When $Y$ can only have two values} we have a \emph{dichotomous} event: the image of $Y$ is $\{0,1\}$. 

When we use a \emph{univariate linear regression} we take it that the relation between $Y_{i}$ and $x_{i}$ can be written as:
\begin{equation}
Y_{i}=\beta_{0} + \beta_{1}x_{i} + \epsilon_{i}
\end{equation}

The error term $\epsilon_{i}$ arises because other, unknown or uncontrollable, variables cause the pairs $(x_{i}, Y_{i})$ not to lie on the theoretical regression line. Because we assume that these errors are $N(0,\sigma)$ distributed it follows that
\begin{equation} 
\label{eq:linear}
E(Y_{i})=\beta_{0} + \beta_{1}x_{i}
\end{equation}
In this particular case where $Y_{i}$ is the result of a Bernouilli experiment with outcomes $0$ and $1$ and a probability $\pi_{i}$ that $Y_{i}=1$ we can calculate the mean value of $Y_{i}$:
\begin{equation} 
\label{eq:pi}
E(Y_{i})=1(\pi_{i}) + 0(1-\pi_{i})=\pi_{i}
\end{equation}

Combining \ref{eq:linear} and \ref{eq:pi} we get:
\begin{equation}
E(Y_{i})=\beta_{0}+\beta_{1}x_{i}=\pi_{i}
\end{equation}

This means that the mean response $E(Y_{i})=\beta_{0}+\beta_{1}x_{i}$ (the \emph{response function}) is no other than the probability that for this value of $x_{i}$ the response variable $Y_{i}$ will be equal to 1. Because probabili-ties are limited to the interval $[0,1]$, the response function has to be capped at the low and at the high end (Figure~\ref{fig:linresp}).

<<label=linresp,fig=TRUE,include=FALSE, echo=FALSE>>=
n <- 10
b0 <- 1
b1 <- 0.3
linreg <- data.frame(x = seq(-n,n,0.1), y=0)
linreg$y <- b0 +b1*linreg$x
for (i in c(1:(20*n+1))) {ifelse(linreg$y[i]<0, linreg$y[i] <- 0, ifelse(linreg$y[i]>1, linreg$y[i] <- 1, linreg$y[i]<-linreg$y[i]))}
ggplot(data=linreg) +
  geom_line(aes(x = x, y = y)) +
  xlim(-1.1*n, 1.1*n) +
  labs(title = "Response function", 
       x = "independent variable x", 
       y = "E(Y)") +
  JT.theme
@

\begin{figure}
\centering
\includegraphics[width=200pt, height=200pt]{LR-linresp}
\caption{Linear response function}
\label{fig:linresp}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

However there are some profound problems with this linear regression model.

\newthought{Problem 1: the linear model will give response values outside the [0,1] range}

The response function is given by:
\begin{equation}
E(Y_{i}) = \beta_{0} + \beta_{1}x_{i} =\pi_{i}
\label{eq:meanlin}
\end{equation}

But we know that all values of $E(Y_{i})$ should be ''0" or ''1" or in between those numbers. This is not always the case when $\beta_{0}$ and $\beta_{1}$ are determined using the least squares method. In Figure~\ref{fig:linresp} we had to cap the function, otherwise it could have had negative values or values above 1.
\newpage
\newthought{Problem 2: the error term $\epsilon_{i}$ is not normally distributed}

The error can only have two values:

\begin{equation}
\begin{split}
\epsilon_{i}&=1-\beta_{0} - \beta_{1}x_{i} \quad if \quad Y_{i}=1 \\
\epsilon_{i}&=-\beta_{0} - \beta_{1}x_{i} \quad if \quad Y_{i}=0
\end{split}
\end{equation}

With only two possible values, $\epsilon_{i}$ is dichotomous and \emph{not} normally distributed.

\newthought{Problem 3: the variance of the error $\epsilon_{i}$ is not constant}

The variance of the error is equal to the variance of the response variable, because
\begin{equation}
var(\epsilon_{i})=var(Y_{i} - \beta_{0} - \beta_{1}x_{i})=var(Y_{i}) \quad because \quad \beta_{0} + \beta_{1}x_{i}=cnst
\end{equation}

The variance of the response variable is:
\begin{equation}
\begin{split}
var(Y_{i}) & \overset{\Delta}{=} E\left[ (Y_{i} - E\left( Y_{i} \right))^{2}  \right] \\
& = (1-\pi_{i})^{2}\pi_{i}+(0-\pi_{i})^{2}(1-\pi_{i})=(1-\pi_{i})\left[ (1-\pi_{i})\pi_{i} + \pi_{i}^{2} \right] \\
& = (1-\pi_{i})\pi_{i}=\left[ 1-E(Y_{i}) \right] E(Y_{i})
\end{split}
\end{equation}

The variance of the response variable $Y_{i}$ (and thus of the error $\epsilon_{i}$) depends on $E(Y_{i})$, the mean of $Y_{i}$, which changes in function of $x_{i}$. Consequently it is not a constant. 

\newthought{Conclusion}
The three problems cited above make it clear that in general, the basic conditions for a linear regression are \emph{not} met when the response variable is dichotomous.

\newpage
\section{''Sigmoid" functions}

\subsection{The Probit Mean Response function}
The response variable $y$ can only take the values ''0" and ''1". Or we should say the \emph{codes} ''0" and ''1". This coding is often based on another, continuous, variable $y'$. For example: the diameter of a aluminium rod can be influenced by the temperature ($x$) at which it is extruded. If the temperature is too low, the diameter will be too large. The rod is qualified as ''OK=1" when its diameter ($y'$) is below 25mm (Figure~\ref{fig:extrude})\sidenote{The coding could of course be inverse when values above cut off get $y=0$, values below cut off get $y=1$}. 

<<label=extrude,fig=TRUE,include=FALSE, echo=FALSE>>=
extrude <- data.frame(x=seq(325, 525, by=25), y=seq(29, 21, by=-1))
set.seed(2018)
extrude$y <- extrude$y + rnorm(9,0,0.2)
p1 <- ggplot(data=extrude) +
  geom_point(aes(x = x, y = y)) +
  geom_hline(yintercept=25, color="red") +
  xlim(300, 550) +
  labs(title = "Diameter of extruded aluminium rods",
       x = "temperature (C)",
       y = "y' = diameter (mm)") +
  JT.theme

beta0 <- -220
beta1 <- 0.5
scurve <- data.frame(x=seq(325, 525, by=2.5), y=0)
scurve$y <- pnorm(beta0+beta1*scurve$x, 0, 1)
lijn <- data.frame(x=c(445, 525), y =c(1,1))
p2 <- ggplot() +
  geom_line(data=scurve, aes(x = x, y = y), linetype="dashed") +
  geom_line(data=lijn, aes(x=x, y=y), color="red") +
  xlim(325, 550) +
  ylim(-0.05, 1.05) +
  labs(title = "Decision curve",
       x = "temperature (C)",
       y = "y") +
  JT.theme
grid.arrange(p1, p2, nrow=2)
@

\begin{figure}
\centering
\includegraphics[width=200pt, height=200pt]{LR-extrude}
\caption{Deciding when $Y_{i}=1$}
\label{fig:extrude}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}


When we call this cut off value for $y'$, $y'_{co}$ and we assume that there is a linear relation between $y'$ and $x$ it follows that:
\begin{equation}
  \begin{split}
    P[Y_{i}=1]=\pi_{i}&=P[Y'_{i}< y'_{co}] \\
    &=P[\beta'_0 + \beta'_{1}X_{i}+\epsilon'_{i} < y'_{co}] \\
    &=P[\epsilon'_{i} < y'_{co} - \beta'_{0} -\beta'_{1}X_{i}] \\
    &=P[\frac{\epsilon'_{i} -0}{\sigma'} < \frac{y'_{co} - \beta'_{0}}{\sigma'} - \frac{\beta'_{1}}{\sigma'}X_{i}] \quad with \quad \epsilon'_{i} \sim \mathcal{N} (0,\sigma') \\
    &=P[Z < (\beta_{0}^{*}+\beta_{1}^{*}X_{i})]
  \end{split}
\end{equation}

With $\phi(z)$ the cumulative density function of the standard normal distribution this gives the \emph{probit mean response function}:
\begin{equation}
E(Y_{i})=\pi_{i}=\phi\left[ \beta_{0}^{*}+\beta_{1}^{*}X_{i}  \right]
\end{equation}

The inverse of this function is the \emph{probit transformation}:
\begin{equation}
\phi^{-1}(\pi_{i})=\beta_{0}^{*}+\beta_{1}^{*}X_{i}
\end{equation}

<<label=probit,fig=TRUE,include=FALSE, echo=FALSE>>=
n <- 10
step <- 0.1
beta0 <- 0
beta1 <- c(0.5,1,5)
probit <- data.frame(x=rep(seq(-n,n,by=step),3), beta0=0, beta1=rep(beta1, each=2*n/step+1))
probit$y <- pnorm(probit$beta0+probit$beta1*probit$x, 0, 1)
p1 <- ggplot(data=probit) +
  geom_line(aes(x = x, y = y, color=factor(beta1))) +
  xlim(-1.1*n, 1.1*n) +
  labs(title = "Probit function",
       subtitle = "Beta1 is a form factor",
       x = "independent variable x",
       y = "probability") +
  JT.theme

beta0 <- c(-5,0,5)
beta1 <- 1
probit <- data.frame(x=rep(seq(-n,n,by=step),3), beta0=rep(beta0, each=2*n/step+1), beta1=beta1)
probit$y <- pnorm(probit$beta0+probit$beta1*probit$x, 0, 1)
p2 <- ggplot(data=probit) +
  geom_line(aes(x = x, y = y, color=factor(beta0))) +
  xlim(-1.1*n, 1.1*n) +
  labs(title = "Probit function",
       subtitle = "Beta0 is a place factor",
       x = "independent variable x",
       y = "probability") +
  JT.theme
grid.arrange(p2, p1, ncol=2)
@

\begin{figure}
\centering
\includegraphics[width=200pt, height=200pt]{LR-probit}
\caption{Probit function for different values of $\beta_{0}^{*}$ and $\beta_{1}^{*}$}
\label{fig:probit}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

Figure~\ref{fig:probit} illustrates the following:
\begin{enumerate}
  \item The \emph{probit mean response function} uses the cumulative density function of the standard normal distribution. This guarantees that the results will always be in the range $[0,1]$
  \item Although we started this example looking only at the cut off point $y'_{co}=25mm$ below which $Y_{i}=1$ we can see in Figure~\ref{fig:probit} that automatically there is also a cut off point below which $Y_{i}=0$.
  \item Parameter $\beta_{0}^{*}$ is a \emph{place} parameter. When $\beta_{0}^{*}=0$ the probability will be $0.5$ for $x=0$. For negative values of $\beta_{0}^{*}$ the curve shifts to the right, for positive values of $\beta_{0}^{*}$ the curve shifts to the left
  \item Parameter $\beta_{1}^{*}$ is a \emph{form} parameter. The higher its value, the steeper the curve switches from ''0" to ''1".
  \item If we had used the opposite coding (''0" when $y'<y'_{co}$) we could call this probability variable $Y^{opp}_{i}$ and $Y^{opp}_{i}=1-Y$. But because $\phi(z)=1-\phi(-z)$ it follows that $P[Y^{opp}_{i}]= P[1-Y] = 1-\phi(\beta_{0}^{*}+\beta_{1}^{*}x)=\phi(-\beta_{0}^{*}-\beta_{1}^{*}x)$. It has the same form, only the sign of the $\beta$'s has changed.
\end{enumerate}

\subsection{The logistic function}
Just as in the case of linear regression we can \emph{see} that there is some sort of connection between $y$ an $x$, which is very clear in the scatterplot on the left of Figure~\ref{fig:distinct}, almost non existent on the right of Figure~\ref{fig:distinct} and a little tentative in the middle. 

We could interpret these data by saying that $E(Y_{i})$ is the result of a Bernouilli experiment\sidenote{Hypothesis 1: $Y_{i}$ is the result of a Bernouilli experiment}. This experiment has two outcomes (''1" and ''0") with the following probabilities:
\begin{equation}
  \begin{split}
  P[E(Y_{i})=1] & =\pi(x_{i}) \\
  P[E(y_{i})=0] &= 1-\pi(x_{i})
  \end{split}
\end{equation}

The probability $P[E(Y_{i})=1]=\pi(x_{i})$ \emph{changes} with $x_{i}$: for small values of $x_{i}$ it will be ''0", for large values of $x_{i}$ it will be ''1", and there is a region $[a \leq x_{i} \leq b]$ where the probability changes from ''0" to ''1". We could define $\pi(x)$ by a linear change in this region:
\begin{equation}
  \begin{split}
  x<a & \quad \pi(x)=0 \\
  a \leq x \leq b & \quad \pi(x)=\frac{(x-a)}{(b-a)} \\
  x>b & \quad \pi(x)=1
  \end{split}
\end{equation}

But this is a crude approximation. An \emph{S-shape} or \emph{sigmoid} seems more appropriate (see Figure~\ref{fig:intro}). A possible function is given by:
\begin{equation}
  \sigma(t)=\frac{e^{t}}{1+e^{t}}=\frac{1}{1+e^{-t}}
\end{equation}

This sigmoid function will be zero for $t \rightarrow -\infty$, will be one for $t \rightarrow +\infty$, and will be $0.5$ for $t=0$. We can make this sigmoid function dependent on the independent variable $x$ by defining a function $t=f(x)$. The simplest function is a linear one:
\begin{equation}
  t = \beta_{0} + \beta_{1}x
\end{equation}

The probability $P[E(y_{i})=1]=\pi(x_{i})$ is then given by the \emph{logistic} function:
\begin{equation}
  \pi(x_{i}) = \frac{1}{1+e^{-(\beta_{0} + \beta_{1}x_{i})}}
\end{equation}

Based on this definition of the logistic function we can define two other useful entities: the \emph{odds} and the \emph{logit function}:
\begin{equation}
  \begin{split}
  odds &= \frac{\pi(x_{i})}{1-\pi(x_{i})} = e^{\beta_0 + \beta_{1}x_{i}} \\
  logit(\pi(x_{i})) &= ln(odds) = ln \left( \frac{\pi(x_{i})}{1-\pi(x_{i})}  \right)=\beta_{0} + \beta_{1}x_{i}
  \end{split}
\end{equation}

\subsection{What does the logistic function look like?}

We can see that
\begin{equation}
\begin{split}
\lim_{x\to -\infty} \frac{1}{1+e^{-(\beta_{0} + \beta_{1}x_{i})}} &= 0 \\
\lim_{x\to \infty} \frac{1}{1+e^{-(\beta_{0} + \beta_{1}x_{i})}} &= 1 \\
\frac{1}{1+e^{-(\beta_{0} + \beta_{1}x_{i})}}(x_{i}=0) &= \frac{1}{1+e^{-\beta_{0}}}
\end{split}
\end{equation}

The parameter $\beta_{0}$ is a \emph{place} factor (Figure~\ref{fig:place_and_form}-left): for positive values of $\beta_{0}$ it shifts the graph to the left, for negative values to the right. Parameter $\beta_{1}$ is a \emph{form} factor (Figure~\ref{fig:place_and_form}-right): for high values of $\beta_{1}$ it squeezes the graph to the center, for low values it streches the graph. Positive values of $\beta_{1}$ make the sigmoid function go from low (0) to high (1), negative values of $\beta_{1}$ make the sigmoid function go from high (1) to low (0).

<<label=place_and_form,fig=TRUE,include=FALSE, echo=FALSE>>=
df_pandf <- data.frame(x=seq(-10,10,0.5),y1=0, y2=0, y3=0, y4=0, y5=0, y6=0, y7=0, y8=0)
df_pandf$y1 <- 1/(1+exp(-5-df_pandf$x))
df_pandf$y2 <- 1/(1+exp(0-df_pandf$x))
df_pandf$y3 <- 1/(1+exp(+5-df_pandf$x))
p1 <- ggplot(data=df_pandf, aes(x=x)) + 
  geom_line(aes(y=y1), color="red") +
  geom_line(aes(y=y2), color="black") +
  geom_line(aes(y=y3), color="blue") +
  labs(title="beta0\nchanges place", xlab="x", ylab="y") +
  geom_text(x=-8, y=0.25, label="+5", color="red") +
  geom_text(x=-3, y=0.25, label="0", color="black") +
  geom_text(x=2.3, y=0.25, label="-5", color="blue") +
  JT.theme
df_pandf$y4 <- 1/(1+exp(0-2.5*df_pandf$x))
df_pandf$y5 <- 1/(1+exp(0-1*df_pandf$x))
df_pandf$y6 <- 1/(1+exp(0-0.25*df_pandf$x))
p2 <- ggplot(data=df_pandf, aes(x=x)) + 
  geom_line(aes(y=y4), color="red") +
  geom_line(aes(y=y5), color="black") +
  geom_line(aes(y=y6), color="blue") +
  geom_text(x=2, y=0.25, label="2.5", color="red") +
  geom_text(x=-2, y=0.25, label="1", color="black") +
  geom_text(x=-8, y=0.25, label="0.25", color="blue") +
  labs(title="beta1\nchanges form", xlab="x", ylab="y") +
  JT.theme
df_pandf$y7 <- 1/(1+exp(0-1*df_pandf$x))
df_pandf$y8 <- 1/(1+exp(0+1*df_pandf$x))
p3 <- ggplot(data=df_pandf, aes(x=x)) + 
  geom_line(aes(y=y7), color="red") +
  geom_line(aes(y=y8), color="blue") +
  geom_text(x=-4, y=0.25, label="+1", color="red") +
  geom_text(x=4, y=0.25, label="-1", color="blue") +
  labs(title="sign of beta1\nchanges direction", x="x", y="y") +
  JT.theme
grid.arrange(p1, p2, p3, ncol=3)
@

\begin{figure}
\centering
\includegraphics[width=1\textwidth, height=250pt]{LR-place_and_form}
\caption{Place and form}
\label{fig:place_and_form}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}


\subsection{Finding the coefficients $\beta_{0}$ and $\beta_{1}$}

In linear regression we start from one set $(x_{i}, y_{i})$ of experimental results and therefore we cannot find the ''true" values of $\beta_{0}$ and $\beta_{1}$: we will find $b_{0}$ and $b_{1}$, one of many possible solutions. The same is true in logistical regression: based on the data of one experiment we will not find $\beta_{0}$ and $\beta_{1}$ but $b_{0}$ and $b_{1}$.\sidenote{Even replicated experiments will always be just a selection of all possible experiments}

In linear regression we find $b_{0}$ and $b_{1}$ using the Ordinary Least Squares method: we \emph{minimise} the error which is the sum of the squares of the residuals:
\begin{equation}
  \sum_{i=1}^{i=n}(y_{i}-\hat{y}_{i})^{2}=\sum_{i=1}^{i=n}(r_{i})^{2} \quad with \quad r_{i}= residual_{i}
\end{equation}

An analytical solution of this problem is available. 

When we accept the hypothesis that ($Y_{i}$ is the result of a Bernouilli experiment with probability $\pi_{i}$ \sidenote{hypothesis 1: $Y_{i}$ is the result of a Bernouilli experiment with probability $\pi_{i}$}, then the probability distribution for $Y_{i}$ can be written as:
\begin{equation}
  P[Y_{i}|x_{i}]=\left[ \pi_{i}  \right]^{Y_{i}}\left[ 1 - \pi_{i}  \right]^{(1 - Y_{i})}
\end{equation}

because:
\begin{equation}
  \begin{split}
  if \quad Y_{i}=1 \quad then \quad P[Y_{i} &=1|x_{i}]=\left[ \pi_{i}  \right].1 \\
  if \quad Y_{i}=0 \quad then \quad P[Y_{i} &=0|x_{i}]=1.\left[ 1 - \pi_{i}  \right]
  \end{split}
\end{equation}

If we assume\sidenote{Hypothesis 2: the observations are statistically independent} that the observations are independent, then the total probability for all observations is equal to the product of all the probabilities. We call this probability the \emph{Likelihood function}:
\begin{equation}
  \mathcal{L}=\prod_{i=1}^{i=n}\left[ \pi_{i}  \right]^{Y_{i}}\left[ 1 - \pi_{i}  \right]^{(1 - Y_{i})}
\end{equation}

For a specific set of $n$ observations $(x_{i},y_{i}), i=1 \ldots n$ this wil be:
\begin{equation}
  \begin{split}
    \mathcal{L} &=\prod_{i=1}^{i=n}\left[ p_{i}  \right]^{y_{i}}\left[ 1 - p_{i}  \right]^{(1 - y_{i})} \\
                &=\prod_{i=1}^{i=n}\left[ \frac{e^{b_0 + b_{1}x_{i}}}{1 + e^{b_0 + b_{1}x_{i}}}  \right]^{y_{i}}\left[ \frac{1}{1 + e^{b_0 + b_{1}x_{i}}}  \right]^{(1 - y_{i})} \\
                &=\prod_{i=1}^{i=n}\frac{e^{y_{i}(b_0 + b_{1}x_{i})}}{1 + e^{b_0 + b_{1}x_{i}}} 
  \end{split}
\end{equation}
Because all probabilities lie between $0$ and $1$, it follows that the product of these probabilities will also have a result between these two limits: the \emph{Likelihood} function will have values between 0 and 1.

By taking the logarithm of $\mathcal{L}$ we obtain the \emph{Log-Likelihood function} $\ell$. Here we can work with sums instead of products, which is numerically advantageous:
\begin{equation}
  \begin{split}
    \ell &= ln(\mathcal{L}) \\
        &=\sum_{i=1}^{i=n}\left[ ln \left( e^{y_{i}(b_0 + b_{1}x_{i})} \right) - ln \left( 1 + e^{(b_0 + b_{1}x_{i})} \right) \right] \\
        &=\sum_{i=1}^{i=n}\left[  y_{i}(b_0 + b_{1}x_{i})   - ln \left( 1 + e^{(b_0 + b_{1}x_{i})} \right) \right] \\
  \end{split}
  \label{eq:loglikelihoodlogit}
\end{equation}

The \emph{Log-Likelihood} function will have values between $-\infty$ and $0$.

In logistical regression the criterium is to find the \emph{maximum} of the likelyhood function. No analytical solution is available and the maximum is found using numerical techniques such as the \emph{Newton-Raphson method}. Essentially it involves a iterative process in which a set of equtions based on the \emph{Hessian matrix} and the \emph{gradient vector} of the \emph{Log Likelihood function}\sidenote{See: "Finding the maximum of the (log) likelihood function with the Newton-Raphson method"}. In R this is done by the \textit{glm(family = binomial(link = "logit"))} function (General Linear Model), where we specify that we assume the process to be ''binomial", and the link function to be ''logit".

\newthought{Example}

The data\sidenote{Two data sets are available: the ''Challenger" data (temperature and o-ring failure), and ''Studytime vs pass" data (relation between hours of study and passing a test)} are given by the following scatterplot (Figure~\ref{fig:scatter}). ''Failure" is coded as ''1" in the ''Challenger" data set, and coded as ''0" in the ''Studytime" data set.
<<echo=FALSE>>=
# Two data sets
study.pass <- data.frame(studytime=c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5), pass=c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1))
data(challenger)
challenger.disaster <- data.frame(temp=challenger[,2],oring=challenger[,1])
@
<<label=scatter,fig=TRUE,include=FALSE, echo=FALSE>>=
data.df <- challenger.disaster
ggplot() + 
  geom_point(aes(x=data.df[,1], y=data.df[,2])) +
  labs(title="Scatterplot", x=colnames(data.df)[1], y=colnames(data.df)[2]) +
  JT.theme
@

\begin{marginfigure}[3cm]
\includegraphics[width=1\textwidth]{LR-scatter}
\caption{Scatterplot of response variable vs. independent variable}
\label{fig:scatter}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

We find the values for $b_{0}$ and $b_{1}$ that maximize the \emph{(Log) Likelihood} function using the \textit{glm}-function in R, ''family=binomial" and ''link function = logit". The result is superimposed on the data in Figure~\ref{fig:scatterplusmodel}.

<<>>=
mod.lr <- glm(data.df[,2]~data.df[,1],data=data.df,
            family=binomial(link="logit"))
tidymod.lr <- broom::tidy(mod.lr)
tidymod.lr
@

<<label=scatterplusmodel,fig=TRUE,include=FALSE, echo=FALSE>>=
model.lr <- data.frame(x=seq(min(data.df[,1]), max(data.df[,1]), length.out = 25), y=0)
model.lr$y <- 1/(1+exp(-(mod.lr$coefficients[1] + mod.lr$coefficients[2]*model.lr$x)))
ggplot() + 
  geom_point(data = data.df, aes(x=data.df[,1], data.df[,2])) + 
  geom_line(data = model.lr, aes(x=x, y=y), colour="red") +
  labs(title="Scatterplot and Logistic regression model", x=colnames(data.df)[1], y=colnames(data.df)[2]) +
  JT.theme 
@

\begin{marginfigure}[1cm]
\includegraphics[width=1\textwidth]{LR-scatterplusmodel}
\caption{Scatterplot and logistic regression model}
\label{fig:scatterplusmodel}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\section{Quality of the regression model}

\subsection{Hypothesis test: the Likelihood Ratio Test}

When we use the ''logit" link function we assume that the probability $P[Y_{i}=1]=\pi(x_{i})$ depends on $x_{i}$:

\begin{equation}
P[Y_{i}=1]=\pi(x_{i})=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x_{i})}}
\end{equation}

This is the alternative hypothesis $H_{a}$. The null-hypothesis $H_{0}$ assumes that the probability is independent of $x_{i}$ which is the same as saying that $\beta_{1}=0$.

The \emph{Log-Likelihood} function under the null-hypothesis is:
\begin{equation}
\ell(H_{0}) = \sum_{i=1}^{i=n} \left[y_{i}b_{0} - ln(1+e^{b_{0}} \right]=\sum_{i=1}^{i=n} y_{i}b_{0} - n.ln(1+e^{b_{0}})
\end{equation}

This is a function of $b_{0}$ and it will have a maximum value for $b_{0}=b_{0}^{*}$ when
\begin{equation}
\frac{d}{db_{0}} \ell(H_{0})(b_{0}^{*})=\sum_{i=1}^{i=n}y_{i}-n \frac{e^{b_{0}^{*}}}{1+e^{b_{0}^{*}}}=0
\end{equation}

Because
\begin{equation}
  \begin{split}
  \frac{\sum_{i=1}^{i=n}y_{i}}{n} &= \frac{e^{b_{0}^{*}}}{1+e^{b_{0}^{*}}} \\
  \hat{y} &= \frac{e^{b_{0}^{*}}}{1+e^{b_{0}^{*}}} \\
  \hat{y} &= (1 - \hat{y})e^{b_{0}^{*}}
  \end{split}
\end{equation}

from which follows that the maximum is reached for the value $b_{0}=b_{0}^{*}$
\begin{equation}
b_{0}^{*}=ln \frac{\bar{y}}{1-\bar{y}}
\end{equation}

The maximum of the \emph{Log Likelihood} function under the null-hypothesis, $\ell_{0}$, is then
\begin{equation}
\ell_{0}=ln(\frac{\bar{y}}{1-\bar{y}})n\bar{y} - n.ln \left[ 1 + ln( \frac{\bar{y}}{1-\bar{y}}) \right]
\end{equation}

The maximum of the \emph{Log Likelihood} function under the alternative hypothesis, $\ell_{a}$, is found by calculating equation \ref{eq:loglikelihoodlogit} with the values $b_{0}=\hat{b}_{0}$ and $b_{1}=\hat{b}_{1}$ that were found numerically:
\begin{equation}
    \ell_{a} =\sum_{i=1}^{i=n}\left[  y_{i}(\hat{b}_0 + \hat{b}_{1}x_{i})   - ln \left( 1 + e^{(\hat{b}_0 + \hat{b}_{1}x_{i})} \right) \right]
\end{equation}

In an ideal case, the value of the \emph{Log Likelihood} function is 0 and the value of the \emph{Likelihood} function is 1. \emph{Deviance} is a measure of the deviation of the model from the ideal and is defined as:
\begin{equation}
D=-2\ell=-2ln \mathcal{L}=ln (\frac{1}{\mathcal{L}^{2})}
\end{equation}

In the ideal case ($\mathcal{L}=1$) the \emph{deviance} is zero, for $\mathcal{L}=0.5$ it will be 1.386, for $\mathcal{L}=0.1$ it is 4.605 etc. 

The \emph{deviances} under the null-hypothesis and under the alternative hypothesis are:
\begin{equation}
\begin{split}
D_{0}&=-2\ell_{0}=ln (\frac{1}{\mathcal{L}_{0}^{2}}) \\
D_{a}&=-2\ell_{a}=ln (\frac{1}{\mathcal{L}_{a}^{2}})
\end{split}
\end{equation}

These \emph{deviances} have respectively $df_{0}=(n-1)$ and $df_{a}=(n-2)$ degrees of freedom: under the null-hypothesis we only lose one degree of freedom by estimating $\beta_{0}$, while under the alternative hypothesis we lose two by estimating $\beta_{0}$ and $\beta_{1}$.

It can be shown that for large sample sizes $n$ the difference $(D_{0}-D_{a})$ has a chi-square distribution with $df_{0}-df_{a}$ degrees of freedom. In this case $df_{0}-df_{a}=1$ and thus:
\begin{equation}
D_{0}-D_{a} \sim \chi^{2}_{(df_{0}-df{a})}=\chi^{2}_{1}
\end{equation}

The difference $(D_{0}-D_{a})$ can be written as:
\begin{equation}
D_{0}-D_{a}=ln (\frac{1}{\mathcal{L}_{0}^{2}}) - ln (\frac{1}{\mathcal{L}_{a}^{2}}) = ln (\frac{\mathcal{L}_{a}^{2}}{\mathcal{L}_{0}^{2}}) = 2 ln (\frac{\mathcal{L}_{a}}{\mathcal{L}_{0}})
\end{equation}

It is twice the (natural) logaritm of the \emph{Likelihood Ratio} with in the numerator the value of the \emph{Likelihood} function under the alternative hypothesis, and in the denominator the value of the \emph{Likelihood} function under the null hypothesis. Therefore this hypothesis test is called the \emph{Likelihood Ratio Test (LRT)}

With this distribution we can calculate the probability of finding the observed value of $D_{0}-D_{a}$ (or greater):
\begin{equation}
p=P[\chi^{2}_{1} \geq D_{0}-D_{a}]=1-pchisq(D_{a}-D_{0},df_{0}-df_{a})
\end{equation}

\newthought{Example}

Calculating the p-value of the Likelihood Ration Test:
<<>>=
# Calculating the value of the Log Likelihood function under the null-hypothesis
n <- nrow(data.df)
bar_y <- mean(data.df[,2])
b0_star <- log(bar_y/(1-bar_y))
l0 <- b0_star*n*bar_y - n* log(1+exp(b0_star))
# Calculating the value of the Log Likelihood function under the alternative hypothesis
b0_hat <- tidymod.lr$estimate[1]
b1_hat <- tidymod.lr$estimate[2]
la <- sum(data.df[,2]*(b0_hat + b1_hat*data.df[,1])) - sum(log(1 + exp(b0_hat + b1_hat*data.df[,1])))
# Calculating deviances
D0 <- -2*l0 
Da <- -2*la 
df0 <- n-1 
dfa <- n-2 
p <- 1 - pchisq(D0-Da,df0-dfa)
print(paste("p=",p))
@

\medskip
All this information can be found in the details of the glm-model:
<<>>=
broom::glance(mod.lr)
@
\medskip
\begin{itemize}
  \item null.deviance=$D_{0}$
  \item deviance=$D_{a}$
  \item df.null=$df_{0}$
  \item df.residual=$df_{a}$
  \item logLik=$l_{a}$ the value of the \emph{Log Likelihood} function of the full model (alternative hypothesis $H_{a}$)
  \item AIC: the Akaike Information Criterion. It is comparable with the adjusted determination coefficient $R_{adj}^{2}$ of the linear regression model. It is helpfull when there are many independent variables because it penalises the increased use of more explanatory variables in the model. The formula is:
  \begin{equation}
  AIC=-2ln\mathcal{L}_{a} + 2k=D_{a}+2k
  \end{equation}
  where $k=$number of estimated parameters in the model (in this example $k=2$)
\end{itemize}

<<>>=
k <- 2
AIC <- Da + 2*k
AIC
@

\begin{itemize}
  \item BIC: Bayesian Information Criterion: an alternative to the AIC.
  \begin{equation}
  BIC=-2ln\mathcal{L}_{a} + k.ln(n)=D_{a}+k.ln(n)
  \end{equation}
\end{itemize}

<<>>=
BIC <- Da + k*log(n)
BIC
@


\medskip
The p-value that we calculated $p=\Sexpr{round(p,4)}$ gives an appreciation of the quality of the \textbf{total} model.

We can also find it by an \textit{anova}-analysis using the option ''LRT:Likelihood Ratio Test":

<<>>=
anova(mod.lr, test="LRT")
@

\subsection{Wald test}
The Wald test gives the p-values for the individual parameters $\beta_{0}$ and $\beta_{1}$
\medskip
<<>>=
library(aod)
wald.test(b=coef(mod.lr),Sigma=vcov(mod.lr),Terms=1)
wald.test(b=coef(mod.lr),Sigma=vcov(mod.lr),Terms=2)
@
\medskip
These are the values that we also find for $\beta_{0}$ and $\beta_{1}$ in the summary of the \textit{glm}-function:
<<>>=
tidy(mod.lr)
@

\section{Using the logistic model for classification: the confusion matrix}

The \emph{logistic model} can be used to give predictions. 

\begin{itemize}
  \item Using \textit{predict(glm.model)} will give us the \emph{logit} function:
    \begin{equation}
    predict(glm.model)=logit\left( \pi(x_{i})  \right)=b_{0} + b_{1}x_{i}
    \end{equation}
  \item Using \textit{predict(glm.model, type="outcomes)} will give us the probabilities $\pi(x_{i})$, i.e. the results of applying the sigmoid link function to the results of the \emph{logit} function:
    \begin{equation}
    predict(glm.model, type="outcomes") = \frac{e^{ b_{0} + b_{1}x_{i}}}{1+e^{ b_{0} + b_{1}x_{i}}} = \frac{1}{1 + e^{-( b_{0} + b_{1}x_{i})}}
    \end{equation}
\end{itemize}

We can switch back to a binary result by using a \emph{cut-off} value $co$ for the probabilities: if $\pi(x_{i})<co$ then we set $Y(x_{i})=0$, if $\pi(x_{i})>co$ then we set $Y(x_{i})=1$\sidenote{You have to decide what happens when $\pi(x_{i})=co$}. 

Then we compare our theoretical classification with the results from the experiment. We group the results into four quadrants of the \emph{confusion matrix} (Table~\ref{tab:co05}): 
\begin{itemize}
  \item quadrant I:  prediction = ''TRUE" and the observation = ''1": True Positive = TP
  \item quadrant II: prediction = ''TRUE" and the observation = ''0": False Positive = FP
  \item quadrant III: prediction = ''FALSE" and the observation = ''1": True Negative = FN
  \item quadrant IV: prediction = ''FALSE" and the observation = ''0": True Negative = TN
\end{itemize}

The words ''TRUE/FALSE" and the observation codes ''0/1" should always be interpreted carefully: in the Challenger case the prediction ''TRUE" is equivalent to saying that the model predicts that the o-ring will fail. An observation coded ''0" means that the o-ring was not damaged.

<<echo=FALSE>>=
co <- 0.5
comp.lr <- data.frame(pred=as.numeric(predict(mod.lr, type="response")>co), obs=data.df[,2], group.name=("TP"), stringsAsFactors=FALSE)
for (i in (1:nrow(comp.lr))) {
  if(comp.lr$pred[i] == 1 & comp.lr$obs[i] == 0) {comp.lr$group.name[i]=as.character("FP")}
  if(comp.lr$pred[i] == 0 & comp.lr$obs[i] == 1) {comp.lr$group.name[i]=as.character("FN")}
  if(comp.lr$pred[i] == 0 & comp.lr$obs[i] == 0) {comp.lr$group.name[i]=as.character("TN")}
}
TP <- sum(comp.lr$group.name=="TP")
FP <- sum(comp.lr$group.name=="FP")
FN <- sum(comp.lr$group.name=="FN")
TN <- sum(comp.lr$group.name=="TN")
@

\begin{margintable}[-8cm]
\begin{tabular}{r c c l}
Prediction$\downarrow$ & 1 & 0 & $\leftarrow$Observation \\
TRUE & TP=$\Sexpr{TP}$ & FP=$\Sexpr{FP}$ &\\
FALSE & FN=$\Sexpr{FN}$ & TN=$\Sexpr{TN}$ &
\end{tabular}
\caption{Confusion matrix; cut-off=0.5}
\label{tab:co05}
\end{margintable}

In R we can create this table with the function \textit{table} but the quadrants are differently alligned:
<<>>=
confusion_matrix <- table(data.df[,2],predict(mod.lr, type="response") > co)
confusion_matrix
@

\begin{margintable}
\begin{tabular}{r c c l}
Observation$\downarrow$ & FALSE & TRUE & $\leftarrow$Prediction \\
0 & TN & FP &\\
1 & FN & TP &
\end{tabular}
\caption{Confusion matrix: configuration in R}
\label{tab:coR}
\end{margintable}

It is clear that the choice of the \emph{cut-off} point is of major influence. We can measure the effect of different values of the \emph{cut-off} point, by calculating the \emph{accuracy} which is the proportion of correct predictions.
\begin{equation}
accuracy = \frac{TP + TN}{n}
\end{equation}

We are trying to get the \emph{accuracy} as high as possible.

In view of the fact that the consequence of a failing o-ring could be (and in reality was) disastrous, we clearly do not want a model that leads to a False Negative: a classification where we predict ''Safe" (which in this case is coded as ''FALSE"), while the reality is that it ''Fails" (coded as a ''1"). 

A False Positive means that the model classifies the situation as ''Fails" (coded as ''TRUE"), while the reality is ''Safe" (coded as ''0"). This is annoying (the launch is delayed until the temperature is higher) and it will cost money, but these consequences are incomparable with the loss of life. 

So in this particular case we aim for a choice of the cut-off point that both maximises Q and minimises the False Negative Rate (FNR). Figure~\ref{fig:QandFNR} shows how \emph{accuracy} and the False Negative Rate (FNR) evolve for different values of the cut-off point.

<<label=QandFNR,fig=TRUE,include=FALSE, echo=FALSE>>=
N <- 100
QandFNR <- data.frame(co = seq (0, 1, length.out = N + 1), Q=rep(0, N+1), FNR=rep(0, N+1))
for (i in seq(1, N+1)) {
  comp.lr <- data.frame(pred=as.numeric(predict(mod.lr, type="response")>=co), obs=data.df[,2], group.name=("TP"), stringsAsFactors=FALSE)
    for (j in (1:nrow(comp.lr))) {
      if(comp.lr$pred[j] == 1 & comp.lr$obs[j] == 0) {comp.lr$group.name[j]=as.character("FP")}
      if(comp.lr$pred[j] == 0 & comp.lr$obs[j] == 1) {comp.lr$group.name[j]=as.character("FN")}
      if(comp.lr$pred[j] == 0 & comp.lr$obs[j] == 0) {comp.lr$group.name[j]=as.character("TN")}
    }
  confusion_matrix <- table(data.df[,2],predict(mod.lr, type="response") >= QandFNR$co[i])
  if (dim(confusion_matrix)[2]==2) {
    QandFNR$Q[i] <- round((confusion_matrix[1,1] + confusion_matrix[2,2])/n,3)
    QandFNR$FNR[i] <- round((confusion_matrix[2,1])/n,3)
  }
  if (dim(confusion_matrix)[2]==1) {
    if(colnames(confusion_matrix)=="TRUE") {
      QandFNR$Q[i] <- round(confusion_matrix[2,1]/n, 3)
      QandFNR$FNR[i] <- 0
    }
    if(colnames(confusion_matrix)=="FALSE") {
      QandFNR$Q[i] <- round(confusion_matrix[1,1]/n, 3)
      QandFNR$FNR[i] <- round(confusion_matrix[2,1]/n, 3)
    }
  }
}
ggplot(data=QandFNR) +
  geom_line(aes(x = co, y = Q ), color="Blue") +
  geom_line(aes(x = co, y = FNR ), color="Red") +
  labs(title="Influence of cut-off point on the Accuracy (Blue)\nand the False Negative Rate (Red)", x="co", y="Q, FNR") +
  JT.theme
@

\begin{marginfigure}[-8cm]
\includegraphics[width=1\textwidth]{LR-QandFNR}
\caption{Influence of the cut-off point on Accuracy and False Negative Rate}
\label{fig:QandFNR}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

Setting the cut-off point to 0 will put the False Negative Rate to zero, but it is not realistic because it effectively means that the prediction for all o-rings will be ''TRUE", which means that we will consider all o-rings to be defective, whatever the temperature! We would never launch a spacecraft. After checking them we will indeed find 7 o-rings (out of 23) to be defective (mostly at low temperatures) but also that there are 16 o-rings that are OK. The \emph{accuracy} would be 7/23=0.304.

Setting the cut-off point at 0.5 will give high \emph{accuracy} and low FNR. However: what is an exceptable FNR in this specific case?
<<echo=FALSE>>=
co <- 1
comp.lr <- data.frame(pred=as.numeric(predict(mod.lr, type="response")>co), obs=data.df[,2], group.name=("TP"), stringsAsFactors=FALSE)
for (i in (1:nrow(comp.lr))) {
  if(comp.lr$pred[i] == 1 & comp.lr$obs[i] == 0) {comp.lr$group.name[i]=as.character("FP")}
  if(comp.lr$pred[i] == 0 & comp.lr$obs[i] == 1) {comp.lr$group.name[i]=as.character("FN")}
  if(comp.lr$pred[i] == 0 & comp.lr$obs[i] == 0) {comp.lr$group.name[i]=as.character("TN")}
}
TP <- sum(comp.lr$group.name=="TP")
FP <- sum(comp.lr$group.name=="FP")
FN <- sum(comp.lr$group.name=="FN")
TN <- sum(comp.lr$group.name=="TN")
Q <- (TP + TN)/n
@

\begin{margintable}
\begin{tabular}{r c c l}
Prediction$\downarrow$ & 1 & 0 & $\leftarrow$Observation \\
TRUE & TP=$\Sexpr{TP}$ & FP=$\Sexpr{FP}$ &\\
FALSE & FN=$\Sexpr{FN}$ & TN=$\Sexpr{TN}$ &
\end{tabular}
\caption{Confusion matrix; cut-off=0.5}
\label{tab:co02}
\end{margintable}

In this case the confusion matrix is given in Table~\ref{tab:co02}: \Sexpr{FN} False Negative and a quality of our classification model: $Accuracy=\Sexpr{round(Q,3)}$.

\newpage
\section{Receiver Operating Characteristic: ROC}
\subsection{Definitions}
This graph  summarizes the performance of a classifier for all possible thresholds. It is generated by plotting the True Positive Rate (TPR, Sensitivity, Recall) (y-axis) against the False Positive Rate (FPR, Aspecificity) (x-axis) as you vary the threshold for assigning observations to a given class.

\textbf{Attention!}. The definitions of the True Positive Rate are:
\begin{equation}
  \begin{split}
    TPR &= \frac{TP}{actual "1"} \neq \frac{TP}{n} \\
    FPR &= \frac{FP}{actual "0"} \neq \frac{FP}{n}
  \end{split}
\end{equation}

\subsection{Reference points in the ROC-plot}
Suppose we have a set of $n=100$ observations with 60\% of them (actual) ''1" and 40\% (actual) ''0". If our classifier makes its predictions based on the throw of a fair coin (p=0.5) it will assign the value ''TRUE" in about 30 cases\sidenote{As the number of observations n grows we will get closer to 50\%.60\%.n cases. In this example we take it to be exactly that number} and ''FALSE" in about 30 cases which are actually a ''1". It will assign the value ''TRUE" in about 20 cases and ''FALSE" in about 20 cases which are actually ''0" (Table~\ref{tab:co03}). The classifier has a $TPR=\frac{30}{60}=0.5$ and a $FPR=\frac{30/60}=0.5$. In the ROC-graph the values $FPR=0.5$ and $TPR=0.5$ give the coordinates of point ''A" (Figure~\ref{fig:ROC1}) that represents a classifier based on simple guesswork.

\begin{margintable}
\begin{tabular}{r c c l}
Prediction$\downarrow$ & 1 & 0 & $\leftarrow$Observation \\
TRUE & TP=30 & FP=20 & \\
FALSE & FN=30 & TN=20 & \\
  &60 &40 & actual
\end{tabular}
\caption{Confusion matrix: p=0.5}
\label{tab:co03}
\end{margintable}

With ''simple guesswork" I mean that the classifier does not really look into the experimental data: it simply uses a binomial distribution with p=0.5 to generate predictions. It acts blind with regard to the actual data. 

<<label=ROC1,fig=TRUE,include=FALSE, echo=FALSE>>=
ROC.df <- data.frame(FPR=seq(0, 1, by=0.1), TPR=seq(0, 1, by=0.1))
ggplot(data=ROC.df, aes(x = FPR, y = TPR )) +
  geom_abline(intercept=0, slope=1, color="Red") +
  geom_point(aes(x = 0.5, y = 0.5 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.52, y=0.5),label="A (p=0.5)", hjust="left", color="Blue") +
  geom_point(aes(x = 0.9, y = 0.9 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.88, y=0.9),label="B (p=0.9)", hjust="right", color="Blue") +
  geom_point(aes(x = 0.1, y = 0.1 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.12, y=0.1),label="C (p=0.1)", hjust="left", color="Blue") +
  geom_point(aes(x = 0, y = 0 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.02, y=0),label="D (p=0)", hjust="left", color="Blue") +
  geom_point(aes(x = 1, y = 1 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.98, y=1),label="E (p=1)", hjust="right", color="Blue") +
  scale_x_continuous(limits=c(0,1)) +
  scale_y_continuous(limits=c(0,1)) +
  labs(title="Reference points in ROC", x="FPR", y="TPR") +
  JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{LR-ROC1}
\caption{Reference points in the ROC}
\label{fig:ROC1}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

The classifier could as easily use a binomial distribution with p=0.9 or p=0.1 to generate predictions.With p=0.9 it will predict ''TRUE" in 90\% of the cases\sidenote{n sufficiently large}, and ''FALSE" in 10\% of the cases, and this both for actual values of ''1" and ''0". The confusion matrix is then given by Table~\ref{tab:co04}. This gives a $TPR=\frac{54}{60}=0.9$ and a $FPR=\frac{36}{40}=0.9$ (point ''B" in Figure~\ref{fig:ROC1}) With p=0.1 this will lead to $TPR=\frac{6}{60}=0.1$ and a $FPR=\frac{4}{40}=0.1$ (point ''C" in Figure~\ref{fig:ROC1}).

\begin{margintable}
\begin{tabular}{r c c l}
Prediction$\downarrow$ & 1 & 0 & $\leftarrow$Observation \\
TRUE & TP=54 & FP=36 &\\
FALSE & FN=6 & TN=4 & \\
  &60 &40 & actual
\end{tabular}
\caption{Confusion matrix: p=0.9}
\label{tab:co04}
\end{margintable}

The red line in Figure~\ref{fig:ROC1} shows the results of ''blind" classifiers: they do not look into the data but simply predict on the basis of a binomial distribution with different values of p.

The extremes are of course for p=0 and p=1. p=0 means that the classifier always predicts ''FALSE". This gives a $TPR=0$ and a $FPR=0$ (point ''D" = the origin of the graph). p=1 always leads to a prediction ''TRUE" with $TPR=1$ and $FPR=1$ (point ''E" = upper right corner).

\subsection{ROC-curves of different classifiers}
Our model (in this case the logistic regression model) should perform better than any blind classifier because it takes into account the observations. A perfect classifier would predict all actual results correctly. This gives Table~\ref{tab:co05} with $TPR=1$ and $FPR=0$ (Point ''F" in Figure~\ref{fig:ROC2}).

\begin{margintable}
\begin{tabular}{r c c l}
Prediction$\downarrow$ & 1 & 0 & $\leftarrow$Observation \\
TRUE & TP=60 & FP=0 &\\
FALSE & FN=0 & TN=40 & \\
  &60 &40 & actual
\end{tabular}
\caption{Confusion matrix: different classifiers}
\label{tab:co05}
\end{margintable}

<<label=ROC2,fig=TRUE,include=FALSE, echo=FALSE>>=
ROC.df <- data.frame(FPR=seq(0, 1, by=0.1), TPR=seq(0, 1, by=0.1))
ggplot(data=ROC.df, aes(x = FPR, y = TPR )) +
  geom_abline(intercept=0, slope=1, color="Red") +
  geom_point(aes(x = 0.5, y = 0.5 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.52, y=0.5),label="A (p=0.5)", hjust="left", color="Blue") +
  geom_point(aes(x = 0.9, y = 0.9 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.88, y=0.9),label="B (p=0.9)", hjust="right", color="Blue") +
  geom_point(aes(x = 0.1, y = 0.1 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.12, y=0.1),label="C (p=0.1)", hjust="left", color="Blue") +
  geom_point(aes(x = 0, y = 0 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.02, y=0),label="D (p=0)", hjust="left", color="Blue") +
  geom_point(aes(x = 1, y = 1 ), shape=21, fill="Blue", size=3) +
  geom_text(aes(x=0.98, y=1),label="E (p=1)", hjust="right", color="Blue") +
  geom_point(aes(x = 0, y = 1 ), shape=21, fill="Green", size=3) +
  geom_text(aes(x=0.02, y=1),label="F (perfect)", hjust="left", color="Black") +
  geom_point(aes(x = 1, y = 0 ), shape=21, fill="Red", size=3) +
  geom_text(aes(x=0.98, y=0),label="G (perfectly wrong)", hjust="right", color="Black") +
  scale_x_continuous(limits=c(0,1)) +
  scale_y_continuous(limits=c(0,1)) +
  labs(title="Different classifiers in a ROC", x="FPR", y="TPR") +
  JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{LR-ROC2}
\caption{Different classifiers in a ROC}
\label{fig:ROC2}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

His counterpart is the really obtuse classifier who gets every prediction wrong: for every actual ''1" he predicts ''FALSE" and for every actual ''0" he predicts ''TRUE". That is represented by point G (1,0). In this case you could argue that this classifier is actually perfect, but it has misunderstood the interpretation of ''FALSE" and ''TRUE".

A classifier that takes into account the observed data should be better than blind guessing. This means that the ROC curve should lie above the diagonal in the area where TPR>FPR. However: many classifier models are possible. In our Challenger example we only took into account one predictior variable: $x=temperature$. It is conceivable that if we were to expand our logisitic regression model taking into account more predictor variables, our classifier could improve. Figure~\ref{fig:ROC3} shows 4 ROC-curves for different logistic regression classifier models.

<<label=ROC3,fig=TRUE,include=FALSE, echo=FALSE>>=
AUC <- data.frame(type=c("a", "b", "c", "d"), AUC=rep(0,4))
FPR <- seq(0,1, length.out = 11)
x <- FPR
curves.df <- data.frame(FPR=rep(FPR, 4), TPR=0, type=rep(c("a", "b", "c", "d"), each=11))
y1 <- sin(x*pi/2)
AUC$AUC[1] <- 2/pi
c <- 0.2
b <- -(1 + 3*c)
a <- -2*b -4*c
y2 <- a*x + b*x^2 + c*x^4
AUC$AUC[2] <- a/2 + b/3 + c/4
c <- 0.5
b <- -(1 + 3*c)
a <- -2*b -4*c
y3 <- a*x + b*x^2 + c*x^4
y3[9:11] <- 1
AUC$AUC[3] <- a/2 + b/3 + c/4
y4 <- c(0, 0.25, 0.25, 0.45, 0.60, 0.85, 0.93, 0.96, 0.97, 0.99, 1)
for (i in c(2:11)) {
  AUC$AUC[4] <- AUC$AUC[4] + 0.05*(y4[i] - y4[i-1]) + 0.1*y4[i-1]
}
curves.df$TPR <- c(y1, y2, y3, y4)
ggplot(data=curves.df) + 
  geom_line(aes(x=FPR, y=TPR, color=type)) +
  geom_abline(intercept=0, slope=1, color="Black", linetype=2) +
  scale_x_continuous(limits=c(0,1)) +
  scale_y_continuous(limits=c(0,1.01)) +
  labs(title="Different ROC-curves", x="FPR", y="TPR") +
  JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{LR-ROC3}
\caption{Different ROC-curves}
\label{fig:ROC3}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

It is clear that the better classifier lies higher above the diagonal and closer to the ideal point in the top left corner. On sight we can conclude that ''c" is better than ''b" is better than ''a". But how to we qualify ''d"? The criterium used to distinguish between the ROC-curves is the \emph{Area Under Curve - AUC}. The lowest value is the area under the diagonal with a value of $AUC=0.5$. The AUC's of the other curves are:
<<>>=
AUC
@

It is clear that ''c" is better than ''b" is better than ''a", but also that ''d" is worse than ''b" but better than ''a".

\subsection{Optimal cut-off point}
The optimal cut-off point will be the one that results in the point on the ROC-curve that is closest to the ''perfect"-classifier, which is the point (0,1). The distance is equal to $\sqrt{FPR^{2}+(1-TPR)^2}$. Finding the minimum of this value will get you the optimal cut-off point.

\subsection{ROC-curve for the Challenger example}
The classifier will lead to different confusion matrices for different choices of the cut-off points. Every choice will have a TPR and an FPR and thus a ''model"-point in the graph. A \emph{low} cut-off point will lead to a lot of ''TRUE"-predictions, which is equivalent to a blind classifier with a high p-value and so we expect to find the coordinates of models with a \emph{low} cut-off point in the top right hand corner of the graph. Models with a \emph{high} cut-off point should fall into the lower left corner of the graph. The ROC-curve of our logistical regression model for different values of the cut-off point is given in Figure~\ref{fig:ROC4}. All points lie above the diagonal, which is good. But most are far away from the ideal classifier.

<<label=ROC4,fig=TRUE,include=FALSE, echo=FALSE>>=
ROC_curve <- data.frame(co=seq(0,1, length.out=11), FPR=c(1, rep(0,10)), TPR=c(1, rep(0,10)), dist=c(1, rep(0,10)))
ROC_curve$co.label <- as.character(ROC_curve$co)
preds <- predict(mod.lr, type="response")
for (i in c(2:10)) {
  comp.lr <- data.frame(pred=as.numeric(preds>ROC_curve$co[i]), obs=data.df[,2])
  ROC_curve$TPR[i] <- sum(comp.lr$pred==1 & comp.lr$obs==1)/sum(comp.lr$obs==1)
  ROC_curve$FPR[i] <- sum(comp.lr$pred==1 & comp.lr$obs==0)/sum(comp.lr$obs==0)
  ROC_curve$dist[i] <- sqrt(ROC_curve$FPR[i]^2 + (1-ROC_curve$TPR[i])^2)
}
ROC_curve$dist[11] <- 1
ROC_curve <- ROC_curve[order(-FPR),]
ggplot(data=ROC_curve, aes(FPR, TPR, label=co.label)) +
  geom_line(color="Red") +
  geom_point(color="Black") +
  geom_text(hjust="left", nudge_x = 0.01, nudge_y=0.02, check_overlap = TRUE, size=3) +
  geom_abline(intercept=0, slope=1, color="Black", linetype=2)+
  scale_x_continuous(limits=c(0,1.05)) +
  scale_y_continuous(limits=c(0,1.05)) +
  labs(title="ROC-curve for logistic regression model", x="FPR", y="TPR") +
  JT.theme
AUC_lr <- 0
for (i in c(2:11)) {
  AUC_lr <- AUC_lr + (1/2)*(ROC_curve$FPR[i]-ROC_curve$FPR[i-1])*(ROC_curve$TPR[i] - ROC_curve$TPR[i-1]) + (ROC_curve$FPR[i]-ROC_curve$FPR[i-1])*ROC_curve$TPR[i-1]
}
@

\begin{marginfigure}[-3cm]
\includegraphics[width=1\textwidth]{LR-ROC4}
\caption{ROC of the logistic regression model}
\label{fig:ROC4}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

The Area Under Curve (AUC) is equal to $\Sexpr{round(AUC_lr,3)}$. The optimal cut-off point, which gives the point on the ROC-curve closest to point (0,1) is given by:

<<>>=
ROC_curve %>% filter(dist==min(dist))
@

The package \textbf{ROCR} has a number of usefull functions with regard to judging the quality of a classifier (Figure~\ref{fig:ROC5}):

<<label=ROC5,fig=TRUE,include=FALSE, echo=TRUE>>=
pr <- prediction(preds, data.df[,2])
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc@y.values[[1]]
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{LR-ROC5}
\caption{ROC using package \textbf{ROCR}}
\label{fig:ROC5}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}


\newpage
\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}