\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Logistic Regression}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=LR}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(tidyverse)
library(gridExtra)
library(scatterplot3d)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions

\frontmatter
\chapter*{Logistic Regression}

\mainmatter
\chapter{Why do we need a logistic regression?}

\section{Y is a discrete variable with two possible values}

Let us start with the situation where the response variable $Y$ can only have two (discrete) levels ''0" and ''1", and that there is only one controllable factor $x$. Let us say that for small values of $x$ the response is ''0", for large values of $x$ the response is ''1". This general description can still give rise to very different situations. It all depends on the region in between: we can go from a situation with a very clear distinction, a fuzzy middle or a fuzzy all round (Figure~\ref{fig:distinct}).

<<label=distinction,fig=TRUE,include=FALSE, echo=FALSE>>=
df_clear <- data.frame(x=seq(-10,10,0.5),y=0)
df_clear$y <- ifelse(df_clear$x<0,0,1)
df_clear$y <- ifelse(df_clear$x==0,0.5,df_clear$y)
p1 <- ggplot(data=df_clear) + 
  geom_point(aes(x=x, y=y), color="red") +
    labs(title="Clear distinction", xlab="x", ylab="y") +
  JT.theme
set.seed(2017)
df_fuzzy_middle <- data.frame(x=seq(-10,10,0.5),y=0)
df_fuzzy_middle$y[17:25] <- rbinom(9,1,0.5)
df_fuzzy_middle$y[26:41] <- 1
p2 <- ggplot(data=df_fuzzy_middle) + 
  geom_point(aes(x=x, y=y), color="red") +
    labs(title="Fuzzy middle", xlab="x", ylab="y") +
  JT.theme
df_fuzzy_complete <- data.frame(x=seq(-10,10,0.5),y=0)
df_fuzzy_middle$y <- rbinom(41,1,0.5)
p3 <- ggplot(data=df_fuzzy_middle) + 
  geom_point(aes(x=x, y=y), color="red") +
    labs(title="Fuzzy all round", xlab="x", ylab="y") +
  JT.theme
grid.arrange(p1, p2, p3, ncol=3)
@

\begin{figure}
\includegraphics[width=1\textwidth, height=250pt]{LR-distinction}
\caption{Some situations}
\label{fig:distinct}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{figure}

Just as in the case of linear regression we can \emph{see} that there is some sort of connection between $y$ an $x$, which is very clear in the scatterplot on the left of Figure~\ref{fig:distinct}, almost non existent on the right of Figure~\ref{fig:distinct} and a little tentative in the middle. But this connection is clearly not linear. An \emph{S-shape} or \emph{sigmoid} seems more appropriate. This shape reminds us of the cumulative distribution of most probability functions. So, instead of working with the \emph{value} $y$ we shift to a stochastic variable $Y$ and we work with the probability $P[Y=y]$. This has the advantage that, as a probability, it will always have values in the interval $[0,1]$. We are looking at the dependence of $Y$ on $x$: for some cases this will be a function with a small region of uncertainty (values of $P[Y=y]\neq 0$ or $P[Y=y]\neq 1$).

\section{What are the problems with a linear regression?}
When the response variable $Y$ in \emph{qualitative} rather than \emph{quantitative}, linear regression is not the best method to link $Y$ with the controllable factors $x_{i}$\sidenote{Y (in capital) is a probability variable, x (lower case) is known}. 

\newthought{When $Y$ can only have two values} we have a \emph{dichotomous} event: the image of $Y$ is $\{0,1\}$, with $P[Y=1]=\pi$ and $P[Y=0]=1-\pi$. We do not know all values of $Y$, but we have a sample (size=$n$) of $n$ pairs $(x_{i}, y_{i})$. However, the probability $\pi$ could be different for different values of $x_{i}$:

\begin{equation}
P[Y_{i}=1]=\pi_{i} \quad and \quad P[Y_{i}=0]=1-\pi_{i}
\end{equation}

The expected value of $Y_{i}$ is than

\begin{equation}
E(Y_{i})=1\pi_{i} + 0(1-\pi_{i})=\pi_{i}
\label{eq:meanpi}
\end{equation}

When we use a \emph{linear regression} we take it that the relation between $Y$ and $x$ can be written as:

\begin{equation}
Y=\beta_{0} + \beta_{1}x
\end{equation}

The pairs $(x_{i}, Y_{i})$ will not neccesarily lie on the theoretical regression line: there will be some error $\epsilon_{i}$

\begin{equation}
Y_{i}=\beta_{0} + \beta_{1}x_{i} + \epsilon_{i}
\end{equation}

\subsection{Problem 1: linear models give values outside the [0,1] range}

When we calculate the expected value of $Y_{i}$ we get:

\begin{equation}
E(Y_{i})=E\left( \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}  \right) = \beta_{0} + \beta_{1}x_{i}
\label{eq:meanlin}
\end{equation}

From \ref{eq:meanpi} and \ref{eq:meanlin} it follows that

\begin{equation}
\pi_{i}=\beta_{0} + \beta_{1}x_{i}
\label{eq:linkpimean}
\end{equation}

This means that all the values of $\beta_{0} + \beta_{1}x_{i}$ should lie between 0 and 1, because $\pi_{i}$ is a probability. This is not always the case when $\beta_{0}$ and $\beta_{1}$ are determined using the least squares method.

\subsection{Problem 2: the error term $\epsilon_{i}$ is not normally distributed}

From \ref{eq:linkpimean} it follows that the error can only have two values:

\begin{equation}
\begin{split}
\epsilon_{i}&=1-\beta_{0} - \beta_{1}x_{i} \quad if \quad Y_{i}=1 \\
\epsilon_{i}&=-\beta_{0} - \beta_{1}x_{i} \quad if \quad Y_{i}=0
\end{split}
\end{equation}

With only two possible values, $\epsilon_{i}$ is dichotomous and \emph{not} normally distributed.

\subsection{Problem 3: the variance of the error $\epsilon_{i}$ is not constant}

The variance of the error is equal to the variance of the response variable, because
\begin{equation}
var(\epsilon_{i})=var(Y_{i}-\pi_{i})=var(Y_{i}) \quad because \quad \pi_{i}=cnst
\end{equation}

The variance of the response variable is:
\begin{equation}
\begin{split}
var(Y_{i}) & \overset{\Delta}{=} E\left( (Y_{i} - E\left( Y_{i} \right))^{2}  \right) \\
& = (1-\pi_{i})^{2}\pi_{i}+(0-\pi_{i})^{2}(1-\pi_{i}) \\
& = (1-\pi_{i})[(1-\pi_{i})\pi_{i}+\pi_{i}^{2}] \\
& = (1-\pi_{i})\pi_{i} \\
& = E(1-Y_{i})E(Y_{i})
\end{split}
\end{equation}

The variance of the response variable $Y_{i}$ (and thus of the error $\epsilon_{i}$) depends on the mean of $Y_{i}$, and consequently is not a constant.

\subsection{Conclusion}
\newthought{The three problems} cited above make it clear that in general, the basic conditions for a linear regression are \emph{not} met when the response variable is dichotomous.
\newpage
\section{Logistic regression}

\subsection{The logit function}

Instead of using a linear relation between the response variable $Y$ and the control variable x, we can use another function. This function should of course remove all, or most, of the problems cited above. The \emph{logit}-function is often used.

\begin{equation}
\begin{split}
E(Y_{i})&= \beta_{0} + \beta_{1}x_{i} \quad (linear) \\
E(Y_{i})&= \frac{e^{\beta_{0}+\beta_{1}x_{i}}}{1+e^{\beta_{0}+\beta_{1}x_{i}}} \quad (logit) 
\end{split}
\end{equation}

From the definition of the logit function it follows that
\begin{equation}
e^{\beta_{0}+\beta_{1}x_{i}}=\frac{E(Y_{i})}{(1-E(Y_{i})}
\end{equation}

This is called the \emph{odds ratio}.

\subsection{What does the logit function look like?}

We can see that
\begin{equation}
\begin{split}
\lim_{x\to -\infty} \frac{e^{\beta_{0}+\beta_{1}x}}{1+e^{\beta_{0}+\beta_{1}x}} &= 0 \\
\lim_{x\to \infty} \frac{e^{\beta_{0}+\beta_{1}x}}{1+e^{\beta_{0}+\beta_{1}x}} &= 1 \\
\frac{e^{\beta_{0}+\beta_{1}x}}{1+e^{\beta_{0}+\beta_{1}x}}(x=0) &= \frac{e^{\beta_{0}}}{1+e^{\beta_{0}}}
\end{split}
\end{equation}

\subsection{Finding the coefficients $\beta_{0}$ and $\beta_{1}$}

In linear regression we determined the coefficients $\beta_{0}$ and $\beta_{1}$ using the \emph{Least Squares} criterium.

\chapter{Pearson: O-ring failure}

\section{Logistic regression}

<<label=scatter,fig=TRUE,include=FALSE, echo=FALSE>>=
O.ring <- data.frame(tempC=c(12, 13, 14, 17, 18, 19, 19, 19, 20, 20.5, 21, 21, 21, 21, 22, 23, 24, 24, 25, 25, 25.5, 26, 27, 27.2), failure=c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0))
O.ring$tempF <- O.ring$tempC*1.8 + 32
ggplot(data=O.ring) + 
  geom_point(aes(x=tempF, y=failure)) +
    labs(title="Scatterplot O-ring failure", xlab="temperature (F)", ylab="failure: 1=yes, 0=no") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{LR-scatter}
\caption{Scatterplot of O-ring failure as a function of temperature}
\label{fig:scatter}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

<<label=scatterplusmodel,fig=TRUE,include=FALSE, echo=FALSE>>=
modF <- glm(failure~tempF,data=O.ring,family=binomial(link="logit"))
modelF <- data.frame(x=seq(52, 82, 1), y=0)
modelF$y <- 1/(1+exp(-(modF$coefficients[1] + modF$coefficients[2]*modelF$x)))
ggplot(data=O.ring, aes(x=tempF, y=failure)) + 
  geom_point() + 
  geom_line(data = modelF, aes(x=x, y=y), colour="red") +
  JT.theme            
@

<<>>=
summary(modF)
@
\begin{marginfigure}
\includegraphics[width=1\textwidth]{LR-scatterplusmodel}
\caption{Scatterplot and logistic regression model}
\label{fig:scatterplusmodel}
\setfloatalignment{b}% forces caption to be bottom-aligned
\end{marginfigure}

\end{document}